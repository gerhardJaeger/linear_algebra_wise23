{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6d3e8a",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matrix, Rational, sqrt, symbols, zeros\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sympy'"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy import Matrix, Rational, sqrt, symbols, zeros\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d690f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear algebra\n",
    "\n",
    "## Session 07: Orthogonal projection; The determinant\n",
    "\n",
    "## Gerhard Jäger\n",
    "\n",
    "### December 7, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45792e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonality\n",
    "\n",
    "Recall: vectors $\\mathbf v$ and $\\mathbf w$ are **orthogonal** if and only if\n",
    "\n",
    "$$\n",
    "\\mathbf v^T \\mathbf w = \\mathbf 0\n",
    "$$\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\\\0\\end{bmatrix}$, $\\begin{bmatrix}0\\\\0 \\\\1\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\end{bmatrix}$, $\\begin{bmatrix}2\\\\-2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\\\ 2\\end{bmatrix}$, $\\begin{bmatrix}-2\\\\-2\\\\2\\end{bmatrix}$\n",
    "\n",
    "- .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81157a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### orthogonal spaces\n",
    "\n",
    "Two vector spaces $\\mathbf V$ and $\\mathbf W$ are orthogonal if and only if\n",
    "\n",
    "$$\n",
    "\\forall \\mathbf v\\in \\mathbf V, \\mathbf w \\in \\mathbf W. \\mathbf v^T\\mathbf w = \\mathbf 0\n",
    "$$\n",
    "\n",
    "**Examples**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\{\\begin{bmatrix}x\\\\0\\end{bmatrix}: x \\in \\mathbb R\\}\\\\[1em]\n",
    "\\mathbf W &= \\{\\begin{bmatrix}0\\\\y\\end{bmatrix}: y \\in \\mathbb R\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These are of course the $x$-axis and $y$-axis of a 2d-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b3386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\{\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}: x,y \\in \\mathbb R\\}\\\\[1em]\n",
    "\\mathbf W &= \\{\\begin{bmatrix}0\\\\0\\\\z\\end{bmatrix}: z \\in \\mathbb R\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These are the $x$-$y$ plane and the $z$-axis of a 3d-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2fcd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\mathrm{span}(\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    ")\\\\[1em]\n",
    "\\mathbf W &= \\mathrm{span}(\\begin{bmatrix}-1\\\\-1\\\\2\\end{bmatrix})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "How do we know whether $\\mathbf V$ and $\\mathbf W$ are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfbf31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observation** Let $V$ and $W$ be two sets of vectors $\\subseteq \\mathbb R^n$. \n",
    "\n",
    "$\\mathrm{span}(V)$ is orthogonal to $\\mathrm{span}(W)$ if and only if for all $\\mathbf v\\in V, \\mathbf w \\in W$: $\\mathbf v$ and $\\mathbf w$ are orthogonal.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Suppose $\\mathrm{span}(V)$ is orthogonal to $\\mathrm{span}(W)$. If $\\mathbf v\\in V$, then $\\mathbf v\\in\\mathrm{span}(V)$, and likewise for $\\mathbf w$. Hence $\\mathbf v$ and $\\mathbf w$ are orthogonal.\n",
    "\n",
    "Now suppose for all $\\mathbf v\\in V, \\mathbf w \\in W$: $\\mathbf v$ and $\\mathbf w$ are orthogonal. Let $\\mathbf x\\in\\mathrm{span}(V)$ and $\\mathbf y\\in\\mathrm{span}(W)$.\n",
    "\n",
    "If $\\mathbf x\\in\\mathrm{span}(V)$ and $\\mathbf y\\in\\mathrm{span}(W)$, $\\mathbf x = \\sum_i r_i\\mathbf v_i$, $\\mathbf y = \\sum_j s_j\\mathbf w_j$ for $r_1,\\ldots,r_{|V|}, s_1,\\ldots,s_{|W|}\\in \\mathbb R$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf x^T\\mathbf y &= (\\sum_i r_i\\mathbf v_i)^T(\\sum_j s_i\\mathbf w_j)\\\\\n",
    "        &= \\sum_i (r_i\\mathbf v_i)^T(\\sum_j s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_j(r_i\\mathbf v_i)^T( s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_jr_i\\mathbf v_i^T( s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_jr_is_j\\mathbf v_i^T\\mathbf w_j\\\\\n",
    "        &= \\sum_i \\sum_jr_is_j\\mathbf 0\\\\\n",
    "        &= \\mathbf 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc598cd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observation**\n",
    "\n",
    "Let $A$ be an $m\\times n$ matrix. Then\n",
    "\n",
    "- the column space $C(A)$ is orthogonal to the left null space $C(A^T)$, and\n",
    "- the row space $C(A^T)$ is orthogonal to the null space $N(A)$.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "The column space of $A$ is $\\mathrm{span}(\\{\\mathbf a_i|1\\leq i \\leq n\\})$. If $\\mathbf x$ is in the left null space of $A$, this means that\n",
    "\n",
    "$$\n",
    "A^T \\mathbf x = \\mathbf 0\n",
    "$$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$\n",
    "\\forall i:\\mathbf a_i^T\\mathbf x = \\mathbf 0\n",
    "$$\n",
    "\n",
    "Due to the previous observation, it follows that $C(A)$ is orthogonal to $N(A^T)$. \n",
    "\n",
    "The proof of the second statement is analogous.\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c7df2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "Suppose we have two vectors $\\mathbf a$ and $\\mathbf b$. We want to find the *orthogonal projection from $\\mathbf a$ onto the line through $\\mathbf b$*. This is a vector $\\mathbf p$ with the properties:\n",
    "\n",
    "- $\\mathbf p = x\\mathbf b$ ($\\mathbf p$ lies on the line defined by $\\mathbf a$)\n",
    "- $\\mathbf a - \\mathbf p$ is orthogonal to $\\mathbf b$\n",
    "\n",
    "Here is how we find $\\mathbf p$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\mathbf a - x\\mathbf b)^T\\mathbf b &= 0\\\\\n",
    "(\\mathbf a^T - x\\mathbf b^T)\\mathbf b &= 0\\\\\n",
    "\\mathbf a^T\\mathbf b - x\\mathbf b^T\\mathbf b &= 0\\\\\n",
    "\\mathbf a^T\\mathbf b &= x\\mathbf b^T\\mathbf b\\\\\n",
    "x &= \\frac{\\mathbf a^T\\mathbf b}{\\mathbf b^T\\mathbf b}\\\\\n",
    "\\mathbf p &= \\frac{\\mathbf a^T\\mathbf b}{\\mathbf b^T\\mathbf b}\\mathbf b\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $\\mathbf p$ is called the *projection of $\\mathbf a$ onto the line throuhg $\\mathbf b$*.\n",
    "- $\\mathbf e = \\mathbf a - \\mathbf p$ is called the *error*.\n",
    "- $\\mathbf p$ is the point on the line through $\\mathbf b$ which is closest to $\\mathbf a$, i.e., the point which minimizes the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23861d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "Now suppose we have a matrix $A$ and a vector $\\mathbf b$, and we want to find the *orthogonal projection of  $\\mathbf b$ onto the* ***column space*** of $A$.\n",
    "\n",
    "In other words, we want to find the point $\\mathbf p$ which\n",
    "\n",
    "- is in the column space of $A$, and\n",
    "- minimizes the error $\\mathbf b-\\mathbf p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0186408",
   "metadata": {},
   "source": [
    "\n",
    "<img src=_img/projection.svg>\n",
    "\n",
    "(image from https://medium.com/linear-algebra/part-17-projections-122aac21b07c)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8467e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- assumptions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A\\mathbf x &= \\mathbf p\\\\\n",
    "\\mathbf p + \\mathbf e &= \\mathbf b\\\\\n",
    "A^T\\mathbf e &= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- finding the solution\n",
    "\n",
    "Let us assume that the columns of $A$ are independent. (If this is not the case, we can replace $A$ by some basis of $C(A)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c9de6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Observation** $(A^TA)$ is invertible if and only if the columns of $A$ are independent.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "\n",
    "Suppose $(A^TA)$ is invertible, and let $A\\mathbf x = \\mathbf 0$. Then it follows\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA\\mathbf x &= A^T\\mathbf 0\\\\\n",
    "A^TA\\mathbf x &= \\mathbf 0\\\\\n",
    "\\mathbf x &= (A^TA)^{-1}\\mathbf 0\\\\\n",
    "&= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "This entails that the columns of $A$ are independent.\n",
    "\n",
    "Now suppose the columns of $A$ are independent. The Gauss-Jordan elimination factorizes\n",
    "\n",
    "$$\n",
    "A^T = E R,\n",
    "$$\n",
    "where $E$ is the combined elimination matrix and $R$ is the reduced row echelon form of $A^T$.\n",
    "\n",
    "As shown earlier, $E$ is invertible.\n",
    "\n",
    "If the columns of $A$ are independent, $R$ contains $n$ pivot columns, and no free column. It follows that\n",
    "\n",
    "$$\n",
    "R^T R = \\mathbf I,\n",
    "$$\n",
    "\n",
    "since the dot product of a pivot column with itself must be $1$, and the dot product of two different pivot columns must be $0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3273f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA &= ERR^T E^T\\\\\n",
    "&= E~\\mathbf I~ E^T\\\\\n",
    "&= E E^T\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By construction, $E$ is invertible. Therefore\n",
    "\n",
    "$$\n",
    "(A^TA)^{-1} = (E^{-1})^T E^{-1}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b67139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- deriving the solution:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A\\mathbf x &= \\mathbf p\\\\\n",
    "\\mathbf p + \\mathbf e &= \\mathbf b\\\\\n",
    "A^T\\mathbf e &= \\mathbf 0\\\\\n",
    "A^T\\mathbf b &= A^T\\mathbf p + A^T\\mathbf e\\\\\n",
    "A^T\\mathbf b &= A^T\\mathbf p\\\\\n",
    "&= A^T A\\mathbf x\\\\\n",
    "\\mathbf x &= (A^T A)^{-1}A^T\\mathbf b\\\\\n",
    "\\mathbf p &= A(A^T A)^{-1}A^T\\mathbf b\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb16e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection matrix\n",
    "\n",
    "The matrix\n",
    "\n",
    "$$\n",
    "P = A(A^TA)^{-1}A^T\n",
    "$$\n",
    "\n",
    "is the **projection matrix** that maps each vector to its projection onto the column space of $A$.\n",
    "\n",
    "Each projection matrix $P$ has the property that $PP=P$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P &= A(A^TA)^{-1}A^T\\\\\n",
    "PP &= A(A^TA)^{-1}A^TA(A^TA)^{-1}A^T\\\\\n",
    "&= A(A^TA)^{-1}(A^TA)(A^TA)^{-1}A^T\\\\\n",
    "&= A(A^TA)^{-1}A^T\\\\\n",
    "&= P\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215042a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Statistics interlude: Linear regression\n",
    "\n",
    "**linear regression**\n",
    "\n",
    "- independent variables: $m\\times n$ matrix $X$\n",
    "    - $n$: number of observations\n",
    "    - $m$: number of independent variables\n",
    "- dependent variable: length-$n$ vector $\\mathbf y$\n",
    "- goal: find parameter vector $\\beta$ (length $m+1$) such that the *total squared error* is minimized\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\arg_\\beta\\min \\sum_i (\\beta_1 + \\sum_{j=1}^m \\beta_{j+1}x_{i,j} - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39345f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's rephrase this with linear algebra\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_1 &= [\\mathbf 1 X]\\\\\n",
    "\\hat {\\mathbf y} &= X_1\\beta\\\\\n",
    "\\epsilon &= ||\\mathbf y - \\hat {\\mathbf y}||^2\\\\\n",
    "\\hat\\beta &= \\arg_\\beta\\min \\epsilon\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the second equation we see that $\\hat {\\mathbf y}$ is in the column space of $X_1$. \n",
    "\n",
    "The goal is to find the point $\\hat {\\mathbf y}$ in the column space of $X_1$ that minimizes the squared distance to $\\mathbf y$.\n",
    "\n",
    "This is also the point that minimizes the absolute distance between $\\hat {\\mathbf y}$ and $\\mathbf y$. In other words, $\\hat {\\mathbf y}$ is the projection of $\\mathbf y$ onto the column space of $X_1$:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = (X_1^TX_1)^{-1}X_1^T\\mathbf y\n",
    "$$\n",
    "\n",
    "If the columns of $X_1$ are not independent, $\\hat\\beta$ is not well-defined (and your statistics software will complain).\n",
    "\n",
    "https://book.stat420.org/collinearity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336f49b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Determinants\n",
    "\n",
    "Consider the matrix \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "What is the area of the parallelogram defined by the two column vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcdce3",
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a, b, c, d = 2, -3, 4, 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "xmin, xmax, ymin, ymax = -9, 9, -1, 9\n",
    "ax.set(xlim=(xmin-1, xmax+1), ylim=(ymin-1, ymax+1), aspect='equal')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks([b,a])\n",
    "ax.set_xticklabels(['b','a'])\n",
    "ax.set_yticks([d,c])\n",
    "ax.set_yticklabels(['d','c'])\n",
    "\n",
    "ax.set_xlabel('x', size=14, labelpad=-24, x=1.03)\n",
    "ax.set_ylabel('y', size=14, labelpad=-21, y=1.02, rotation=0)\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "\n",
    "\n",
    "ax.quiver((0,),(0,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((b,),(d,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((0,),(0,), (b,), (d,), units=\"xy\", scale=1, color='green')\n",
    "ax.quiver((a,),(c,), (b,), (d,), units=\"xy\", scale=1, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288eb6f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### horizontal elimination\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\frac{ad-bc}{d} & b \\\\\n",
    "0 & d\n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\frac{ad-bc}{d} & 0 \\\\\n",
    "0 & d\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057de7d",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "a, b, c, d = 8, -3, 0, 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "xmin, xmax, ymin, ymax = -9, 9, -1, 9\n",
    "ax.set(xlim=(xmin-1, xmax+1), ylim=(ymin-1, ymax+1), aspect='equal')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks([-3, 2])\n",
    "ax.set_xticklabels(['b','a'])\n",
    "ax.set_yticks([2, 4])\n",
    "ax.set_yticklabels(['d','c'])\n",
    "\n",
    "ax.set_xlabel('x', size=14, labelpad=-24, x=1.03)\n",
    "ax.set_ylabel('y', size=14, labelpad=-21, y=1.02, rotation=0)\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "\n",
    "\n",
    "ax.quiver((0,),(0,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((b,),(d,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((0,),(0,), (b,), (d,), units=\"xy\", scale=1, color='green')\n",
    "ax.quiver((a,),(c,), (b,), (d,), units=\"xy\", scale=1, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d716c1",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a, b, c, d = 8, 0, 0, 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "xmin, xmax, ymin, ymax = -9, 9, -1, 9\n",
    "ax.set(xlim=(xmin-1, xmax+1), ylim=(ymin-1, ymax+1), aspect='equal')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks([-3,2])\n",
    "ax.set_xticklabels(['b','a'])\n",
    "ax.set_yticks([2,4])\n",
    "ax.set_yticklabels(['d','c'])\n",
    "\n",
    "ax.set_xlabel('x', size=14, labelpad=-24, x=1.03)\n",
    "ax.set_ylabel('y', size=14, labelpad=-21, y=1.02, rotation=0)\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "\n",
    "\n",
    "ax.quiver((0,),(0,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((b,),(d,), (a,), (c,), units=\"xy\", scale=1, color='red')\n",
    "ax.quiver((0,),(0,), (b,), (d,), units=\"xy\", scale=1, color='green')\n",
    "ax.quiver((a,),(c,), (b,), (d,), units=\"xy\", scale=1, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d276d2a",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "The area of this rectangle is\n",
    "\n",
    "$$\n",
    "ad-bc\n",
    "$$\n",
    "\n",
    "Since the geometric transformations  corresponding to elimination did not change the area, the original parallelogram also has the area $ad-bc$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61eebd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Determinants\n",
    "\n",
    "The determinant of a matrix $A$ is written as\n",
    "$$\n",
    "\\displaystyle\\mathrm{det}(A)\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\displaystyle|A|\n",
    "$$\n",
    "\n",
    "The determinant is a function that maps each *square matrix* to a *real number*.\n",
    "\n",
    "### three ways to compute the determinant\n",
    "\n",
    "- axiomatic approach\n",
    "- via permutations (\"Leibniz' formula\")\n",
    "- recursively via cofactors (\"Laplace expansion\")\n",
    "\n",
    "The absolute value of the determinant equals the area/volume of the parallelepiped defined by the column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Axiomatix approach\n",
    "\n",
    "#### 1. For each $n$, $$|\\mathbf I_n| = 1$$ where $\\mathbf I_n$ is the $n\\times n$ identity matrix.\n",
    "\n",
    "#### 2. If $B$ is the result of swapping two columns in $A$, then\n",
    "$$\n",
    "|B| = - |A|\n",
    "$$\n",
    "\n",
    "#### 3. $|A|$ is a linear function of each of its columns:\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "|&\\cdots & |&\\cdots & |\\\\\n",
    "\\mathbf a_1 & \\cdots &x\\mathbf a_i & \\cdots & \\mathbf a_n\\\\\n",
    "|&\\cdots & |&\\cdots & | \\\\\n",
    "\\end{vmatrix} = \n",
    "x\\begin{vmatrix}\n",
    "|&\\cdots & |&\\cdots & |\\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_i & \\cdots & \\mathbf a_n\\\\\n",
    "|&\\cdots & |&\\cdots & | \\\\\n",
    "\\end{vmatrix}\\\\[3em]\n",
    "\\begin{vmatrix}\n",
    "|&\\cdots & |&\\cdots & |\\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_i + \\mathbf b_i & \\cdots & \\mathbf a_n\\\\\n",
    "|&\\cdots & |&\\cdots & | \\\\\n",
    "\\end{vmatrix} = \n",
    "\\begin{vmatrix}\n",
    "|&\\cdots & |&\\cdots & |\\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_i & \\cdots & \\mathbf a_n\\\\\n",
    "|&\\cdots & |&\\cdots & | \\\\\n",
    "\\end{vmatrix} +\n",
    "\\begin{vmatrix}\n",
    "|&\\cdots & |&\\cdots & |\\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf b_i & \\cdots & \\mathbf a_n\\\\\n",
    "|&\\cdots & |&\\cdots & | \\\\\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "As we will see later, there is exactly one function with these properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927bb60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consequences\n",
    "\n",
    "From these three axioms, we can derive seven useful lemmas.\n",
    "\n",
    "#### 4. If two columns of $A$ are identical, $|A| = 0$.\n",
    "\n",
    "*Proof:* Swapping the two identical columns does not change $A$, but according to Axiom 2, we get $|A| = -|A|$. The only value consistent with this is $|A| = 0$.\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adae814",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5. Adding a multiple of one column of $A$ to another column does not change $|A|$.\n",
    "\n",
    "*Proof:* According to rules 3 and 4:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& \\mathbf a_i + x\\mathbf a_j&\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix} &= \n",
    "\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& \\mathbf a_i &\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix} +\n",
    "\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& x\\mathbf a_j &\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix}\\\\[2em]\n",
    "&=\n",
    "\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& \\mathbf a_i &\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix} +\n",
    "x\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& \\mathbf a_j &\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix}\\\\[2em]\n",
    "&= \n",
    "\\begin{vmatrix}\n",
    "| & \\cdots & | & | & | & \\cdots & | \\\\\n",
    "\\mathbf a_1 & \\cdots &\\mathbf a_{i-1}& \\mathbf a_i &\\mathbf a_{i+1} & \\cdots & \\mathbf a_n\\\\\n",
    "| & \\cdots & | & | & |& \\cdots & |\\\\\n",
    "\\end{vmatrix} \\\\[2em]\n",
    "&=|A|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f937b44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 6. If $A$ has an all-zero column, then $|A| = 0$.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "If $A$ has more than one all-zero columns, this follows from rule 4. If it has only one all-zero column, then adding one of the other columns to the all-zero column does not change the determinant according to rule 5. This will lead to a matrix with two identical columns though, which has a determinant $=0$ according to rule 4. \n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8836f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 7. The determinant of a triangular matrix equals the product of its diagonal entries.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "Let us first assume that all diagonal entries are $\\neq 0$. \n",
    "\n",
    "According to rule 5, we can perform mirrored Gauss elimination (working with columns rather than with rows) with $A$ if it is upper triangular, and otherwise mirrored Jordan elimination. According to rule 5 this does not change the determinant, and according to the rules of Gauss-Jordan elimination, this does not change the diagonal entries. We end up with a diagonal matrix\n",
    "\n",
    "$$\n",
    "D = \\begin{bmatrix}\n",
    "d_{11} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & d_{ii}\\\\\n",
    "&&&\\ddots\\\\\n",
    "&&&&d_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using Axiom 2, it follows that\n",
    "\n",
    "$$\n",
    "|D| = d_{11}\\times \\cdots \\times d_{nn}\\times |\\mathbf I_n| = d_{11}\\times \\cdots \\times d_{nn}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e1333",
   "metadata": {},
   "source": [
    "Now suppose $d_{ii}=0$. Then the product of the diagonal entries $=0$. \n",
    "\n",
    "After completing mirrorred Gauss-Jordan elimination, the last column is all-zero. Elimination steps do not change the determinant, so the determinant must be $0$ according to rule 6.\n",
    "\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e58fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Uniqueness of the determinant function**\n",
    "\n",
    "With these rules, we can compute the determinant of each square matrix by performing Gauss elimination (possibly preceded by column permutations) and computing the product of the resulting triangular matrix. This algorithm only relies on the three axioms, so any function fulfilling the axioms must yield the same result for each matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da63879",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 8. Product rule: for all square matrices $A, B$ of the same size:\n",
    "\n",
    "$$\n",
    "|AB| = |A| \\times |B|\n",
    "$$\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "Suppose $|A| = 0$. Gauss elimination does not change the determinant. If we need to perform column permutation prior to (mirrorred) Gauss elimination, this only changes the sign of the determinant, not its absolute value.\n",
    "Since $|A|=0$, the result of Gauss elimination must, according to Lemma 7, contain at least one $0$. Hence $A$ must have rank $r<n$ (where $n$ is the number of columns of $A$). In other words, there is a vector $\\mathbf x\\neq \\mathbf 0$ such that $\\mathbf x'A = \\mathbf 0$. It follows that $\\mathbf x'AB = \\mathbf 0$, so $AB$ also has rank $< n$. This entails that the result of Gauss elimination applied to $AB$ has a $0$ on the main diagonal, so $|AB| = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3346b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now suppose $|A| \\neq 0$. Let us define a function \n",
    "\n",
    "$$\n",
    "D(B) \\doteq \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{|AB|}{|A|}&\\mbox{ if }|A|\\neq 0\\\\\n",
    "|B| &\\mbox{ else}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The function $D$ fulfills axioms 1–3:\n",
    "\n",
    "- If $B = \\mathbf I$, then $D(B) = \\frac{|A|}{|A|} = 1$\n",
    "- If $C$ is the result of multiplying column $i$ in $B$ by $x$, then there is a matrix $M$, which is exactly like the identity matrix except $m_{ii} = x$, with:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C &= BM\\\\\n",
    "AC &= ABM\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So $AC$ is the result of multiplying column $i$ in $(AB)$ by $x$. Hence\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "|AC|&= |ABM|\\\\\n",
    "&= x|AB|\\\\\n",
    "D(C) &= xD(B)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671c504",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "- If $C$ is the result of swapping columns $i$ and $j$ in $B$, then\n",
    "\n",
    "$$\n",
    "C = B P,\n",
    "$$\n",
    "\n",
    "Where $P$ is like the identiy matrix, except that $p_{ij}, p_{ji} = 1, p_{ii}, p_{jj} = 0$.\n",
    "\n",
    "Then \n",
    "$$\n",
    "AC =  ABP,\n",
    "$$\n",
    "\n",
    "hence $AC$ is the result of swapping columns $i$ and $j$ in $AB$. Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "|AC| &= |ABP|\\\\\n",
    "&= - |AB|\\\\\n",
    "D(C) &= - D(B)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore $D(C) = - D(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42eaf0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since there is only one function that fulfills axioms 1–3, $D(B) = |B|$. It follows that if $|A|\\neq 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D(B) &=& \\frac{|AB|}{|A|} &= |B|\\\\\n",
    "&&|AB| &= |A| \\times |B|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb721c83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 9. $|A|\\neq 0$ if and only if $A$ is invertible.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "Suppose $|A|\\neq 0$. Then Gauss-Jordan elimination ends in the identity matrix, and the product of the elimination steps amount to $A^{-1}$. \n",
    "\n",
    "If $A$ is invertible, $A^{-1}$ exists, with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A A^{-1} &= \\mathbf I\\\\\n",
    "|A A^{-1}| &= |\\mathbf I|\\\\\n",
    "|A|\\times |A^{-1}|&= 1\\\\\n",
    "|A| &= \\frac{1}{|A^{-1}|}\\\\\n",
    "&\\neq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159a067",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 10. $|A| = |A^T|$\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "It follows directly from rule 7 that for triangular matrices $M$, $|M| = |M^T|$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A &= LU\\\\\n",
    "|A| &= |L| \\times |U|\\\\\n",
    "&= |U| \\times |L|\\\\\n",
    "&= |U^T| \\times |L^T|\\\\\n",
    "&= |U^TL^T|\\\\\n",
    "&= |(LU)^T|\\\\\n",
    "&= |A^T|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\dashv$\n",
    "\n",
    "It follows from rules 5 and 10 that adding a multiple of one row to another row does not change the determinant. So all elimination steps keep the determinant unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488b573",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takehome messages\n",
    "\n",
    "1. The fastest general way to compute the determinant of a matrix is by\n",
    "    - performing Gauss elimination\n",
    "    - computing the product of the diagonal entries.\n",
    "\n",
    "2. A square matrix is invertible if and only if its determinant $\\neq 0$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
